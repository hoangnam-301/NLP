# B√ÅO C√ÅO LAB 6


## I. M·ª•c ti√™u 

M·ª•c ti√™u c·ªßa b√†i lab n√†y l√† x√¢y d·ª±ng v√† so s√°nh hi·ªáu nƒÉng gi·ªØa nhi·ªÅu m√¥ h√¨nh ph√¢n lo·∫°i √Ω ƒë·ªãnh (Intent Classification) kh√°c nhau, bao g·ªìm:

1. **TF-IDF + Logistic Regression** (m√¥ h√¨nh baseline truy·ªÅn th·ªëng)  
2. **Word2Vec trung b√¨nh + Dense Layer**  
3. **LSTM v·ªõi Embedding Word2Vec pretrained**  
4. **LSTM h·ªçc Embedding t·ª´ ƒë·∫ßu**

T·∫≠p d·ªØ li·ªáu s·ª≠ d·ª•ng l√† **HWU64**, g·ªìm h∆°n 10.000 c√¢u thu·ªôc 64 l·ªõp √Ω ƒë·ªãnh kh√°c nhau trong c√°c lƒ©nh v·ª±c nh∆∞ l·ªãch, th·ªùi ti·∫øt, nh·∫°c, email, ƒëi·ªÅu khi·ªÉn thi·∫øt b·ªã IoT, v.v.

---

## II. K·∫øt qu·∫£ ƒë·ªãnh l∆∞·ª£ng

| **Pipeline** | **F1-score (Macro)** | **Test Loss** |
|---------------|----------------------|---------------|
| TF-IDF + Logistic Regression | **0.839** | N/A |
| Word2Vec (Avg) + Dense | **0.686** | 0.983 |
| Embedding (Pre-trained) + LSTM | **0.002** | 3.943 |
| Embedding (Scratch) + LSTM | **0.104** | 2.976 |

### Nh·∫≠n x√©t:
- **TF-IDF + Logistic Regression** ho·∫°t ƒë·ªông r·∫•t t·ªët, F1-score cao v√† ·ªïn ƒë·ªãnh nh·∫•t.  
- **Word2Vec + Dense** ƒë·∫°t k·∫øt qu·∫£ kh√°, cho th·∫•y vi·ªác d√πng embedding gi√∫p m√¥ h√¨nh h·ªçc ƒë∆∞·ª£c m·ªëi quan h·ªá ng·ªØ nghƒ©a gi·ªØa c√°c t·ª´.  
- Hai m√¥ h√¨nh **LSTM (Pretrained & Scratch)** cho k·∫øt qu·∫£ th·∫•p do:
  - D·ªØ li·ªáu hu·∫•n luy·ªán h·∫°n ch·∫ø, m√¥ h√¨nh LSTM ph·ª©c t·∫°p n√™n b·ªã **underfitting**.
  - Embedding pretrained kh·ªüi t·∫°o ch∆∞a ƒë∆∞·ª£c t·ªëi ∆∞u ‚Äî nhi·ªÅu t·ª´ trong tokenizer kh√¥ng c√≥ trong Word2Vec.
  - Dropout v√† recurrent_dropout cao khi·∫øn m√¥ h√¨nh kh√≥ h·ªôi t·ª•.

---

## III. Ph√¢n t√≠ch ƒë·ªãnh t√≠nh 

Ba c√¢u ki·ªÉm th·ª≠ ƒëi·ªÉn h√¨nh:

| **C√¢u v√≠ d·ª•** | **√ù ƒë·ªãnh ƒë√∫ng** | **K·∫øt qu·∫£ m√¥ h√¨nh** |
|----------------|----------------|----------------------|
| ‚Äúcan you remind me to not call my mom‚Äù | `reminder_create` | TF-IDF ‚Üí `calendar_set` ; Word2Vec ‚Üí `social_post` ; LSTM ‚Üí `iot_hue_lightoff` |
| ‚Äúis it going to be sunny or rainy tomorrow‚Äù | `weather_query` | TF-IDF ‚Üí `weather_query` ; Word2Vec ‚Üí `weather_query` ; LSTM ‚Üí `alarm_set` |
| ‚Äúfind a flight from new york to london but not through paris‚Äù | `flight_search` | TF-IDF ‚Üí `transport_ticket` ; Word2Vec ‚Üí `transport_query` ; LSTM ‚Üí `alarm_set` |

### Ph√¢n t√≠ch:

1. **TF-IDF + LR** x·ª≠ l√Ω t·ªët c√°c m·∫´u ng·∫Øn ho·∫∑c c√≥ t·ª´ kh√≥a m·∫°nh (‚Äúweather‚Äù, ‚Äúrainy‚Äù). Tuy nhi√™n, m√¥ h√¨nh **kh√¥ng hi·ªÉu ng·ªØ c·∫£nh ph·ªß ƒë·ªãnh ho·∫∑c ph·ª• thu·ªôc xa** (‚Äúnot through paris‚Äù).  
   ‚Üí K·∫øt qu·∫£ ‚Äútransport_ticket‚Äù l√† h·ª£p l√Ω v·ªÅ ch·ªß ƒë·ªÅ, nh∆∞ng ch∆∞a ƒë√∫ng √Ω ƒë·ªãnh c·ª• th·ªÉ.

2. **Word2Vec + Dense** hi·ªÉu t·ªët h∆°n m·ªëi quan h·ªá gi·ªØa c√°c t·ª´ c√≥ nghƒ©a g·∫ßn nhau.  
   ‚Üí ‚Äúweather_query‚Äù ƒë∆∞·ª£c nh·∫≠n di·ªán ƒë√∫ng v√¨ embedding ƒë√£ h·ªçc ƒë∆∞·ª£c m·ªëi quan h·ªá gi·ªØa c√°c t·ª´ ‚Äúsunny‚Äù, ‚Äúrainy‚Äù, ‚Äútomorrow‚Äù.  
   Tuy nhi√™n, v·∫´n kh√≥ n·∫Øm b·∫Øt c·∫•u tr√∫c c√∫ ph√°p d√†i.

3. **LSTM (Pre-trained / Scratch)** ƒë√°ng ra ph·∫£i x·ª≠ l√Ω t·ªët c√°c **ph·ª• thu·ªôc ng·ªØ c·∫£nh xa** (v√≠ d·ª• ‚Äúnot call my mom‚Äù), nh∆∞ng do embedding y·∫øu v√† d·ªØ li·ªáu nh·ªè ‚Üí m√¥ h√¨nh kh√¥ng h·ªçc ƒë∆∞·ª£c chu·ªói ng·ªØ nghƒ©a ‚Üí d·ª± ƒëo√°n sai.  
   N·∫øu ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë√∫ng c√°ch, LSTM c√≥ th·ªÉ n·∫Øm b·∫Øt quan h·ªá ‚Äúnot + verb‚Äù t·ªët h∆°n c√°c m√¥ h√¨nh truy·ªÅn th·ªëng.

---

## IV. So s√°nh ∆∞u ‚Äì nh∆∞·ª£c ƒëi·ªÉm c·ªßa c√°c ph∆∞∆°ng ph√°p

| **Ph∆∞∆°ng ph√°p** | **∆Øu ƒëi·ªÉm** | **Nh∆∞·ª£c ƒëi·ªÉm** |
|------------------|-------------|----------------|
| **TF-IDF + Logistic Regression** | D·ªÖ hu·∫•n luy·ªán, nhanh, √≠t overfitting, ho·∫°t ƒë·ªông ·ªïn v·ªõi d·ªØ li·ªáu nh·ªè. | Kh√¥ng hi·ªÉu ng·ªØ c·∫£nh, kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c t·ª´ ƒë·ªìng nghƒ©a hay ph·ªß ƒë·ªãnh. |
| **Word2Vec (Avg) + Dense** | Hi·ªÉu ng·ªØ nghƒ©a t·ªët h∆°n, ƒë∆°n gi·∫£n, h·ªôi t·ª• nhanh. | M·∫•t th·ª© t·ª± t·ª´, kh√¥ng n·∫Øm ƒë∆∞·ª£c c√∫ ph√°p hay quan h·ªá xa. |
| **LSTM (Pre-trained)** | C√≥ kh·∫£ nƒÉng h·ªçc ph·ª• thu·ªôc xa, t·∫≠n d·ª•ng embedding pretrained. | D·ªÖ underfitting n·∫øu d·ªØ li·ªáu √≠t; c·∫ßn tinh ch·ªânh embedding k·ªπ. |
| **LSTM (Scratch)** | Linh ho·∫°t, c√≥ th·ªÉ h·ªçc embedding ri√™ng ph√π h·ª£p domain. | Hu·∫•n luy·ªán l√¢u, y√™u c·∫ßu d·ªØ li·ªáu l·ªõn, d·ªÖ overfitting ho·∫∑c kh√¥ng h·ªôi t·ª•. |

---

## V. K·∫øt lu·∫≠n v√† h∆∞·ªõng c·∫£i thi·ªán

- C√°c m√¥ h√¨nh truy·ªÅn th·ªëng (**TF-IDF, Word2Vec**) v·∫´n ho·∫°t ƒë·ªông r·∫•t t·ªët tr√™n t·∫≠p d·ªØ li·ªáu nh·ªè v√† ƒëa l·ªõp.
- C√°c m√¥ h√¨nh **LSTM c·∫ßn th√™m d·ªØ li·ªáu v√† fine-tuning embedding t·ªët h∆°n** (v√≠ d·ª•: s·ª≠ d·ª•ng GloVe ho·∫∑c FastText thay v√¨ Word2Vec t·ª± hu·∫•n luy·ªán).
- Ngo√†i ra, c√≥ th·ªÉ th·ª≠ **m√¥ h√¨nh Transformer (BERT ho·∫∑c DistilBERT)** ƒë·ªÉ c·∫£i thi·ªán ƒë·ªô hi·ªÉu ng·ªØ c·∫£nh v√† ph·ªß ƒë·ªãnh.

üìà V·ªõi c·∫£i ti·∫øn preprocessing, fine-tuning embedding, v√† gi·∫£m dropout, LSTM d·ª± ki·∫øn c√≥ th·ªÉ ƒë·∫°t F1-score > **0.70**, v∆∞·ª£t Word2Vec Dense.

---

## T√†i li·ªáu tham kh·∫£o
- HWU64 Dataset for Intent Classification.  
- Mikolov et al. (2013). *Distributed Representations of Words and Phrases and their Compositionality.*  
- Hochreiter & Schmidhuber (1997). *Long Short-Term Memory.*  
- Scikit-learn, TensorFlow, Gensim documentation.

---

